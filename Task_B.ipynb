{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6947b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "##from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers import Activation, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b59fa6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "830f7b9e-2777-4102-ae8f-edbb10dedde9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_encoding_init(file_names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(file_names)\n",
    "    dump(le, 'Task B Assets/Task_B_label_encoder.joblib') \n",
    "\n",
    "def SMOTE_preprocessing(file_names, labels):\n",
    "    le = joblib.load('Task B Assets/Task_B_label_encoder.joblib')\n",
    "    le.fit(file_names)\n",
    "    le_file_names = le.transform(file_names).reshape(-1,1)\n",
    "    print(le_file_names.shape)\n",
    "    \n",
    "    sm = SMOTE()\n",
    "    SMOTE_file_names, SMOTE_labels = sm.fit_resample(le_file_names, labels)\n",
    "    print(\"Distribution of image categories post-SMOTE:\")\n",
    "    print(Counter(list(SMOTE_labels)))\n",
    "    \n",
    "    SMOTE_file_names = le.inverse_transform(SMOTE_file_names)\n",
    "    print('a')\n",
    "    return SMOTE_file_names, SMOTE_labels\n",
    "\n",
    "def image_processing(file_names):\n",
    "    dataset_tumor=[]\n",
    "    for file_name in file_names:\n",
    "        file=cv2.imread(data_path+\"/image/\"+file_name)\n",
    "        file_resize=cv2.resize(file,(128,128))/255.\n",
    "        dataset_tumor.append(file_resize)\n",
    "    tumor_data = np.array(dataset_tumor)\n",
    "    return tumor_data\n",
    "\n",
    "def preprocessing_data(data_path, file, status):\n",
    "\n",
    "    data=pd.read_csv(data_path+file)\n",
    "    file_names=list(data['file_name'])\n",
    "    labels=data['label'].values.reshape(-1,1)\n",
    "    \n",
    "    if status == 'training':\n",
    "        ohe = OneHotEncoder(handle_unknown = \"ignore\", sparse=False)\n",
    "        ohe = ohe.fit(labels)\n",
    "        \n",
    "        print(\"Distribution of image categories:\")\n",
    "        print(Counter(list(data['label'])))\n",
    "    \n",
    "        label_encoding_init(file_names)\n",
    "        file_names, labels = SMOTE_preprocessing(file_names, labels)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(file_names,labels,test_size=0.2)\n",
    "        print('b')\n",
    "        x_train = image_processing(x_train)\n",
    "        x_test = image_processing(x_test)\n",
    "        print('c')\n",
    "        y_train = ohe.transform(np.array(y_train).reshape(-1,1))\n",
    "        y_test = ohe.transform(np.array(y_test).reshape(-1,1))\n",
    "        dump(ohe, 'Task B Assets/Task_B_one-hot_encoder.joblib') \n",
    "\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    \n",
    "    elif status == 'testing':\n",
    "        ohe = joblib.load('Task B Assets/Task_B_one-hot_encoder.joblib')\n",
    "        y_test = ohe.transform(labels)\n",
    "        x_test = image_processing(file_names) \n",
    "                              \n",
    "        return x_test, y_test\n",
    "    \n",
    "\n",
    "def define_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Conv2D is a 2D convolutional layer that applies 2d convolution to input signal composed of several input planes.\n",
    "    #Kernel: is the filter that moves over the input layer to obtain a matrix of dot products based on the kernel size.\n",
    "    #Strides metric: determines the shift of the kernel filter as it convolves around the input volume. \n",
    "    #Higher stride means larger shifts which lead to a smaller output volume. \n",
    "    #It is important to for the stride to be small enough that we capture significant features but not too small as this could lead to overfitting.\n",
    "    #Padding: refers to the amount of pixels added to an image when it is being processed by the kernel. \n",
    "    #This helps provide more space for the kernel to cover the image.\n",
    "    #Activation: refers to activation function used to product output layer. Here I use ReLU as it is simple and doesn't suffer from vanishing gradients\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(31,31), strides=(1,1), padding='same', activation='relu', input_shape=(128,128,3)))\n",
    "    #MaxPooling reduces the size of the output matrix by obtaining the largest value from the a given pooling matrix \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #Batch Normalization helps reduce the internal covariate shift of the network. \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "              \n",
    "    model.add(Conv2D(64, kernel_size=(27,27), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "              \n",
    "    model.add(Conv2D(128, kernel_size=(21,21), strides=(2,2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "              \n",
    "    #Flatten layer converts data into a 1-dimensional array for inputting it to the next layer. \n",
    "    #We flatten the output of the convolutional layers to create a single long feature vector \n",
    "    #So basically after our layers of pooling, we want to extract the data so we can feed it into our neural network\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(learning_rate=0.001),  metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "'''\n",
    "def train_CNN(x_train,y_train):\n",
    "    model = define_CNN()\n",
    "    history1 = model.fit(x_train,y_train,epochs=50,batch_size=32,shuffle=True,validation_split=0.1)\n",
    "    \n",
    "\n",
    "    plt.plot(history1.history['acc'])\n",
    "    plt.plot(history1.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history1.history['loss'])\n",
    "    plt.plot(history1.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    model.save('Task B Assets/Task_B_CNN_Model')\n",
    "    \n",
    "    return model\n",
    "'''\n",
    "def train_CNN(x_train, y_train):\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    k_number = 0\n",
    "    \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1score = []\n",
    "    \n",
    "    \n",
    "    print(\"CNN training with 5-Fold Cross Validation.\")\n",
    "    for train_index, test_index in kf.split(x_train_val):\n",
    "        k_number += 1\n",
    "        x_train, x_val = x_train_val[train_index], x_train_val[test_index]\n",
    "        y_train, y_val = y_train_val[train_index], y_train_val[test_index]\n",
    "        \n",
    "        model = define_CNN()\n",
    "        \n",
    "        \n",
    "        history1 = model.fit(x_train,y_train,epochs=50,batch_size=32,shuffle=True,validation_split=0.1)\n",
    "        \n",
    "        acc_history = history1.history['acc']\n",
    "        val_acc_history = history1.history['val_acc']\n",
    "        loss_history = history1.history['loss']\n",
    "        val_loss_history = history1.history['val_loss']\n",
    "\n",
    "        print(\"The highest validation acc is {}\".format(np.max(val_acc_history )))\n",
    "\n",
    "        result=model.predict(x_val)\n",
    "        result_class = tf.one_hot(np.argmax(result, axis=1), depth = 4)\n",
    "        \n",
    "        ohe = joblib.load('Task B Assets/Task_B_one-hot_encoder.joblib')\n",
    "        result_class = ohe.inverse_transform(result_class)\n",
    "        y_val_class = ohe.inverse_transform(y_val)\n",
    "\n",
    "        val_accuracy.append(accuracy_score(result_class, y_val_class))\n",
    "        val_precision.append(precision_score(result_class, y_val_class,average='micro'))\n",
    "        val_f1score.append(f1_score(result_class, y_val_class,average='micro'))\n",
    "        val_recall.append(recall_score(result_class, y_val_class,average='micro'))\n",
    "\n",
    "        average_val_accuracy=sum(val_accuracy)/len(val_accuracy)\n",
    "        average_val_precision=sum(val_precision)/len(val_precision)\n",
    "        average_val_recall=sum(val_recall)/len(val_recall)\n",
    "        average_val_f1score=sum(val_f1score)/len(val_f1score)\n",
    "        \n",
    "        print(\"CNN 5-Fold CV:\")\n",
    "        print(\"Average Acc: %.4f\" %(average_val_accuracy))\n",
    "        print(\"Average Precision: %.4f\" %(average_val_precision))\n",
    "        print(\"Average recall: %.4f\" %(average_val_recall))\n",
    "        print(\"Average F1 Score: %.4f \\n\" %(average_val_f1score))\n",
    "        \n",
    "    model.save('Task B Assets/Task_B_CNN_Model')\n",
    "\n",
    "\n",
    "def test_model(x_test, y_test):\n",
    "    print(y_test.shape)\n",
    "    loaded_model = keras.models.load_model('Task B Assets/Task_B_CNN_Model')\n",
    "    result=loaded_model.predict(x_test)\n",
    "    result_class = tf.one_hot(np.argmax(result, axis=1), depth = 4)\n",
    "    #y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "    \n",
    "    ohe = joblib.load('Task B Assets/Task_B_one-hot_encoder.joblib')\n",
    "    result_class = ohe.inverse_transform(result_class)\n",
    "    y_test_class = ohe.inverse_transform(y_test)\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(result_class, y_test_class)\n",
    "    print(\"Accuracy for test data:\", acc)\n",
    "    \n",
    "    plt.figure(figsize = (7,7))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_class, result_class, cmap = 'Blues')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    print(loaded_model.summary())\n",
    "    print(classification_report(y_test_class, result_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c34bc3-5fde-4c15-9790-9a813d6a13a0",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d38e45f7-bc00-45e5-a8db-5dac1b5e5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of image categories:\n",
      "Counter({'glioma_tumor': 860, 'meningioma_tumor': 855, 'pituitary_tumor': 831, 'no_tumor': 454})\n",
      "(3000, 1)\n",
      "Distribution of image categories post-SMOTE:\n",
      "Counter({'meningioma_tumor': 860, 'no_tumor': 860, 'glioma_tumor': 860, 'pituitary_tumor': 860})\n",
      "a\n",
      "b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "data_path=\"dataset/\"\n",
    "file = \"label.csv\"\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = preprocessing_data(data_path, file, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07e464f0-fcc0-4928-a63a-728c1d8a2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2752, 128, 128, 3)\n",
      "(2752, 4)\n",
      "(688, 128, 128, 3)\n",
      "(688, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_val.shape)\n",
    "print(y_train_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130be1d-1a6d-4949-9c9f-762da351875f",
   "metadata": {},
   "source": [
    "## Training and Validation (KFold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "080b3c2b-5d86-4668-99bd-0dc27db78aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN training with 5-Fold Cross Validation.\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 5s 71ms/step - loss: 2.1568 - acc: 0.3960 - val_loss: 6.0944 - val_acc: 0.2036\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 4s 69ms/step - loss: 1.2087 - acc: 0.4631 - val_loss: 1.4048 - val_acc: 0.3032\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 4s 69ms/step - loss: 1.1463 - acc: 0.4975 - val_loss: 1.2073 - val_acc: 0.4253\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 4s 69ms/step - loss: 1.1113 - acc: 0.5131 - val_loss: 1.1157 - val_acc: 0.5339\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 4s 69ms/step - loss: 1.0842 - acc: 0.5470 - val_loss: 1.2078 - val_acc: 0.4977\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 1.0338 - acc: 0.5722 - val_loss: 1.1193 - val_acc: 0.5385\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 4s 69ms/step - loss: 1.0223 - acc: 0.5803 - val_loss: 1.0651 - val_acc: 0.5747\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.9645 - acc: 0.6081 - val_loss: 1.0792 - val_acc: 0.5339\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.9575 - acc: 0.6167 - val_loss: 0.9235 - val_acc: 0.6335\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.9090 - acc: 0.6379 - val_loss: 1.8970 - val_acc: 0.2760\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.8856 - acc: 0.6515 - val_loss: 1.8005 - val_acc: 0.2941\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.8606 - acc: 0.6601 - val_loss: 1.1945 - val_acc: 0.4163\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.8413 - acc: 0.6783 - val_loss: 0.8784 - val_acc: 0.6742\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.8099 - acc: 0.6955 - val_loss: 1.0989 - val_acc: 0.5475\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.7604 - acc: 0.7091 - val_loss: 0.8726 - val_acc: 0.6561\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.7551 - acc: 0.7293 - val_loss: 0.8622 - val_acc: 0.6787\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.7142 - acc: 0.7308 - val_loss: 0.8455 - val_acc: 0.7149\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.6852 - acc: 0.7545 - val_loss: 0.9872 - val_acc: 0.5747\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.6726 - acc: 0.7495 - val_loss: 0.8102 - val_acc: 0.7149\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.6682 - acc: 0.7626 - val_loss: 0.7588 - val_acc: 0.7647\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5756 - acc: 0.8030 - val_loss: 0.7223 - val_acc: 0.7783\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5656 - acc: 0.7949 - val_loss: 0.6803 - val_acc: 0.7873\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5755 - acc: 0.7949 - val_loss: 0.7612 - val_acc: 0.7602\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5533 - acc: 0.8136 - val_loss: 0.6443 - val_acc: 0.7964\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5230 - acc: 0.8217 - val_loss: 0.7723 - val_acc: 0.7964\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.4912 - acc: 0.8313 - val_loss: 0.7812 - val_acc: 0.7738\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.4919 - acc: 0.8318 - val_loss: 0.7526 - val_acc: 0.8190\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.4152 - acc: 0.8586 - val_loss: 0.7573 - val_acc: 0.8190\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.4251 - acc: 0.8540 - val_loss: 0.7805 - val_acc: 0.7919\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3911 - acc: 0.8687 - val_loss: 0.7660 - val_acc: 0.7738\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3787 - acc: 0.8667 - val_loss: 0.7374 - val_acc: 0.8190\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3670 - acc: 0.8692 - val_loss: 0.8405 - val_acc: 0.8009\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3575 - acc: 0.8788 - val_loss: 0.8068 - val_acc: 0.8100\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3576 - acc: 0.8732 - val_loss: 0.9918 - val_acc: 0.7873\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3697 - acc: 0.8783 - val_loss: 0.7669 - val_acc: 0.8100\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3270 - acc: 0.8869 - val_loss: 0.9031 - val_acc: 0.7964\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3301 - acc: 0.8879 - val_loss: 1.0846 - val_acc: 0.7783\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2904 - acc: 0.8889 - val_loss: 0.9669 - val_acc: 0.8054\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3189 - acc: 0.8848 - val_loss: 0.8010 - val_acc: 0.7647\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2926 - acc: 0.8869 - val_loss: 0.9384 - val_acc: 0.7873\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2880 - acc: 0.8899 - val_loss: 1.2588 - val_acc: 0.7557\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2760 - acc: 0.8899 - val_loss: 0.9144 - val_acc: 0.8190\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2591 - acc: 0.8980 - val_loss: 1.0317 - val_acc: 0.7873\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3100 - acc: 0.8833 - val_loss: 1.2046 - val_acc: 0.5792\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2722 - acc: 0.8990 - val_loss: 1.0795 - val_acc: 0.7466\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2795 - acc: 0.8960 - val_loss: 0.9394 - val_acc: 0.8190\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2650 - acc: 0.8985 - val_loss: 1.0186 - val_acc: 0.8235\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2357 - acc: 0.9056 - val_loss: 1.0396 - val_acc: 0.7873\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2059 - acc: 0.9111 - val_loss: 1.1242 - val_acc: 0.8100\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.2304 - acc: 0.8955 - val_loss: 1.0737 - val_acc: 0.7964\n",
      "The highest validation acc is 0.8235294222831726\n",
      "CNN 5-Fold CV:\n",
      "Average Acc: 0.8185\n",
      "Average Precision: 0.8185\n",
      "Average recall: 0.8185\n",
      "Average F1 Score: 0.8185 \n",
      "\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 5s 73ms/step - loss: 2.1193 - acc: 0.4051 - val_loss: 10.7703 - val_acc: 0.2579\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 1.2271 - acc: 0.4677 - val_loss: 2.2895 - val_acc: 0.2217\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 1.1579 - acc: 0.4803 - val_loss: 1.2597 - val_acc: 0.3937\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.1689 - acc: 0.4859 - val_loss: 1.1145 - val_acc: 0.5385\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.1337 - acc: 0.4980 - val_loss: 1.0378 - val_acc: 0.5792\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.1094 - acc: 0.4995 - val_loss: 1.0513 - val_acc: 0.4842\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.0688 - acc: 0.5207 - val_loss: 1.1187 - val_acc: 0.5339\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.0201 - acc: 0.5556 - val_loss: 1.2698 - val_acc: 0.4570\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 1.0048 - acc: 0.5798 - val_loss: 1.0899 - val_acc: 0.5158\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.9708 - acc: 0.6141 - val_loss: 1.3085 - val_acc: 0.4208\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.9577 - acc: 0.6157 - val_loss: 1.2672 - val_acc: 0.5158\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.9032 - acc: 0.6525 - val_loss: 1.0239 - val_acc: 0.5792\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.8563 - acc: 0.6687 - val_loss: 0.9505 - val_acc: 0.6290\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.8588 - acc: 0.6889 - val_loss: 0.8694 - val_acc: 0.7059\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.8319 - acc: 0.6970 - val_loss: 0.9565 - val_acc: 0.5973\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.7797 - acc: 0.7177 - val_loss: 0.9884 - val_acc: 0.6968\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.7366 - acc: 0.7338 - val_loss: 0.7974 - val_acc: 0.7195\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.6995 - acc: 0.7419 - val_loss: 0.9873 - val_acc: 0.6606\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.7133 - acc: 0.7495 - val_loss: 0.8735 - val_acc: 0.6833\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.6432 - acc: 0.7753 - val_loss: 0.8023 - val_acc: 0.7602\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.6430 - acc: 0.7783 - val_loss: 0.9829 - val_acc: 0.7240\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.6122 - acc: 0.7869 - val_loss: 0.8487 - val_acc: 0.7466\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5523 - acc: 0.8126 - val_loss: 0.7102 - val_acc: 0.7511\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.5530 - acc: 0.8157 - val_loss: 0.8514 - val_acc: 0.7602\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.5298 - acc: 0.8177 - val_loss: 1.0378 - val_acc: 0.7466\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.5165 - acc: 0.8303 - val_loss: 0.8185 - val_acc: 0.7919\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.4796 - acc: 0.8348 - val_loss: 1.0387 - val_acc: 0.7692\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.4784 - acc: 0.8404 - val_loss: 0.9935 - val_acc: 0.7466\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.4252 - acc: 0.8520 - val_loss: 0.9470 - val_acc: 0.8009\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.4135 - acc: 0.8601 - val_loss: 0.8468 - val_acc: 0.8190\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.4004 - acc: 0.8631 - val_loss: 0.8804 - val_acc: 0.7964\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3881 - acc: 0.8712 - val_loss: 0.8868 - val_acc: 0.8054\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3990 - acc: 0.8520 - val_loss: 1.0318 - val_acc: 0.7602\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3601 - acc: 0.8763 - val_loss: 1.0884 - val_acc: 0.7330\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3454 - acc: 0.8823 - val_loss: 1.1607 - val_acc: 0.7828\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3431 - acc: 0.8768 - val_loss: 0.9610 - val_acc: 0.8145\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 4s 70ms/step - loss: 0.3221 - acc: 0.8838 - val_loss: 0.9655 - val_acc: 0.7692\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3301 - acc: 0.8808 - val_loss: 0.8978 - val_acc: 0.7783\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.3304 - acc: 0.8712 - val_loss: 1.0428 - val_acc: 0.8054\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2962 - acc: 0.8939 - val_loss: 1.0542 - val_acc: 0.8009\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2715 - acc: 0.8869 - val_loss: 1.0798 - val_acc: 0.8009\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2664 - acc: 0.9010 - val_loss: 1.1477 - val_acc: 0.7692\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2944 - acc: 0.8934 - val_loss: 0.9992 - val_acc: 0.7828\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2547 - acc: 0.8995 - val_loss: 1.0036 - val_acc: 0.8145\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2290 - acc: 0.9051 - val_loss: 1.1071 - val_acc: 0.8054\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2209 - acc: 0.9061 - val_loss: 1.2270 - val_acc: 0.7828\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2276 - acc: 0.9045 - val_loss: 1.2732 - val_acc: 0.7828\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2422 - acc: 0.9076 - val_loss: 1.5129 - val_acc: 0.7602\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2198 - acc: 0.9066 - val_loss: 1.3305 - val_acc: 0.7919\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 4s 71ms/step - loss: 0.2101 - acc: 0.9045 - val_loss: 1.4293 - val_acc: 0.7964\n",
      "The highest validation acc is 0.8190045356750488\n",
      "CNN 5-Fold CV:\n",
      "Average Acc: 0.8203\n",
      "Average Precision: 0.8203\n",
      "Average recall: 0.8203\n",
      "Average F1 Score: 0.8203 \n",
      "\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 13s 203ms/step - loss: 2.0454 - acc: 0.3983 - val_loss: 3.2007 - val_acc: 0.2579\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 1.1937 - acc: 0.4695 - val_loss: 1.4537 - val_acc: 0.2172\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1290 - acc: 0.5129 - val_loss: 1.1331 - val_acc: 0.5294\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1148 - acc: 0.5124 - val_loss: 1.1656 - val_acc: 0.5566\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0934 - acc: 0.5285 - val_loss: 1.0357 - val_acc: 0.5611\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0570 - acc: 0.5603 - val_loss: 1.0262 - val_acc: 0.5611\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0359 - acc: 0.5684 - val_loss: 0.9789 - val_acc: 0.5973\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9864 - acc: 0.5775 - val_loss: 1.0504 - val_acc: 0.5701\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9825 - acc: 0.5871 - val_loss: 1.1381 - val_acc: 0.5113\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9244 - acc: 0.6108 - val_loss: 0.9771 - val_acc: 0.5882\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9074 - acc: 0.6275 - val_loss: 0.8477 - val_acc: 0.6561\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8699 - acc: 0.6381 - val_loss: 0.9400 - val_acc: 0.6290\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8433 - acc: 0.6598 - val_loss: 0.9580 - val_acc: 0.6742\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8076 - acc: 0.6855 - val_loss: 0.8894 - val_acc: 0.6154\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7806 - acc: 0.6996 - val_loss: 0.8016 - val_acc: 0.6425\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7452 - acc: 0.7148 - val_loss: 0.8109 - val_acc: 0.7104\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7507 - acc: 0.7052 - val_loss: 0.8595 - val_acc: 0.6878\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7145 - acc: 0.7395 - val_loss: 0.7161 - val_acc: 0.7330\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7091 - acc: 0.7400 - val_loss: 0.6801 - val_acc: 0.7557\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 12s 202ms/step - loss: 0.6608 - acc: 0.7602 - val_loss: 1.1828 - val_acc: 0.6109\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.6242 - acc: 0.7754 - val_loss: 0.6357 - val_acc: 0.8009\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.6146 - acc: 0.7845 - val_loss: 0.9162 - val_acc: 0.7376\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5477 - acc: 0.8031 - val_loss: 0.7531 - val_acc: 0.7783\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5787 - acc: 0.7971 - val_loss: 0.6730 - val_acc: 0.8100\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5069 - acc: 0.8218 - val_loss: 0.7345 - val_acc: 0.7376\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5104 - acc: 0.8284 - val_loss: 0.6402 - val_acc: 0.8326\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4768 - acc: 0.8430 - val_loss: 0.6792 - val_acc: 0.7964\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4180 - acc: 0.8536 - val_loss: 0.6348 - val_acc: 0.8326\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4541 - acc: 0.8425 - val_loss: 0.8389 - val_acc: 0.7195\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3942 - acc: 0.8551 - val_loss: 0.6754 - val_acc: 0.8235\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.3957 - acc: 0.8582 - val_loss: 0.6477 - val_acc: 0.7828\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3676 - acc: 0.8728 - val_loss: 0.7977 - val_acc: 0.7919\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3391 - acc: 0.8763 - val_loss: 0.6616 - val_acc: 0.8281\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3641 - acc: 0.8672 - val_loss: 1.0075 - val_acc: 0.7692\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3394 - acc: 0.8859 - val_loss: 0.5847 - val_acc: 0.8416\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2930 - acc: 0.8910 - val_loss: 1.0086 - val_acc: 0.7421\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.3028 - acc: 0.8910 - val_loss: 0.8095 - val_acc: 0.7828\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.3031 - acc: 0.8854 - val_loss: 0.6101 - val_acc: 0.8462\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.3200 - acc: 0.8824 - val_loss: 0.6581 - val_acc: 0.8100\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.3080 - acc: 0.8854 - val_loss: 0.7959 - val_acc: 0.8281\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2804 - acc: 0.8910 - val_loss: 0.7771 - val_acc: 0.8190\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2841 - acc: 0.8849 - val_loss: 1.0893 - val_acc: 0.7557\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2636 - acc: 0.8950 - val_loss: 0.7246 - val_acc: 0.8326\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2524 - acc: 0.8894 - val_loss: 0.7709 - val_acc: 0.8100\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2507 - acc: 0.8920 - val_loss: 0.9467 - val_acc: 0.8190\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2567 - acc: 0.8930 - val_loss: 0.8413 - val_acc: 0.8416\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2221 - acc: 0.8955 - val_loss: 0.9664 - val_acc: 0.8462\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2269 - acc: 0.8980 - val_loss: 0.8991 - val_acc: 0.8326\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2217 - acc: 0.9006 - val_loss: 0.8974 - val_acc: 0.8371\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2123 - acc: 0.9046 - val_loss: 0.8073 - val_acc: 0.8416\n",
      "The highest validation acc is 0.8461538553237915\n",
      "CNN 5-Fold CV:\n",
      "Average Acc: 0.8160\n",
      "Average Precision: 0.8160\n",
      "Average recall: 0.8160\n",
      "Average F1 Score: 0.8160 \n",
      "\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 13s 203ms/step - loss: 1.9966 - acc: 0.4276 - val_loss: 12.0394 - val_acc: 0.2036\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.2070 - acc: 0.4629 - val_loss: 2.3542 - val_acc: 0.3032\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.2319 - acc: 0.4614 - val_loss: 1.1506 - val_acc: 0.5294\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1176 - acc: 0.5114 - val_loss: 1.0920 - val_acc: 0.5385\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1115 - acc: 0.5401 - val_loss: 1.0625 - val_acc: 0.5611\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0923 - acc: 0.5295 - val_loss: 1.1578 - val_acc: 0.4887\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0709 - acc: 0.5613 - val_loss: 1.0670 - val_acc: 0.5928\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0479 - acc: 0.5472 - val_loss: 1.1190 - val_acc: 0.5158\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9939 - acc: 0.5790 - val_loss: 1.0239 - val_acc: 0.6109\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9740 - acc: 0.5926 - val_loss: 1.0550 - val_acc: 0.5475\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9457 - acc: 0.6315 - val_loss: 0.9707 - val_acc: 0.5973\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9293 - acc: 0.6310 - val_loss: 0.9914 - val_acc: 0.5747\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9100 - acc: 0.6376 - val_loss: 0.9679 - val_acc: 0.6199\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8680 - acc: 0.6517 - val_loss: 0.9434 - val_acc: 0.6606\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8534 - acc: 0.6658 - val_loss: 0.9499 - val_acc: 0.6335\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8032 - acc: 0.6865 - val_loss: 0.9622 - val_acc: 0.5747\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.8080 - acc: 0.6769 - val_loss: 1.0629 - val_acc: 0.6063\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7892 - acc: 0.7042 - val_loss: 0.9321 - val_acc: 0.6787\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7381 - acc: 0.7304 - val_loss: 0.8391 - val_acc: 0.7104\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.6944 - acc: 0.7511 - val_loss: 0.9834 - val_acc: 0.6154\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.7276 - acc: 0.7299 - val_loss: 0.8721 - val_acc: 0.7149\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.6574 - acc: 0.7718 - val_loss: 0.8214 - val_acc: 0.7376\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.6344 - acc: 0.7764 - val_loss: 0.7852 - val_acc: 0.7421\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.5947 - acc: 0.7956 - val_loss: 0.8480 - val_acc: 0.7330\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5994 - acc: 0.7920 - val_loss: 0.8918 - val_acc: 0.6878\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5513 - acc: 0.8082 - val_loss: 0.9796 - val_acc: 0.6833\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5347 - acc: 0.8233 - val_loss: 0.9090 - val_acc: 0.7376\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.5162 - acc: 0.8289 - val_loss: 0.9405 - val_acc: 0.7376\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4865 - acc: 0.8339 - val_loss: 0.9558 - val_acc: 0.6923\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4645 - acc: 0.8420 - val_loss: 0.9386 - val_acc: 0.6878\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4478 - acc: 0.8400 - val_loss: 0.9675 - val_acc: 0.7376\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4231 - acc: 0.8511 - val_loss: 0.9420 - val_acc: 0.7511\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4293 - acc: 0.8516 - val_loss: 0.8721 - val_acc: 0.7783\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.4096 - acc: 0.8642 - val_loss: 1.2343 - val_acc: 0.5928\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3745 - acc: 0.8682 - val_loss: 0.9963 - val_acc: 0.7466\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3577 - acc: 0.8768 - val_loss: 0.9109 - val_acc: 0.7692\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3573 - acc: 0.8688 - val_loss: 1.0476 - val_acc: 0.7557\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3207 - acc: 0.8889 - val_loss: 1.0966 - val_acc: 0.7376\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3186 - acc: 0.8854 - val_loss: 0.9576 - val_acc: 0.7738\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3202 - acc: 0.8864 - val_loss: 1.2216 - val_acc: 0.7738\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3175 - acc: 0.8804 - val_loss: 1.2677 - val_acc: 0.7104\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2915 - acc: 0.8915 - val_loss: 1.1963 - val_acc: 0.7285\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2677 - acc: 0.8930 - val_loss: 1.5173 - val_acc: 0.7376\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.3020 - acc: 0.8905 - val_loss: 1.3034 - val_acc: 0.7557\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2574 - acc: 0.9001 - val_loss: 1.4003 - val_acc: 0.7149\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2657 - acc: 0.8935 - val_loss: 0.9879 - val_acc: 0.7783\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2497 - acc: 0.9061 - val_loss: 1.1735 - val_acc: 0.7557\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2523 - acc: 0.9011 - val_loss: 1.1180 - val_acc: 0.7828\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.2326 - acc: 0.9046 - val_loss: 1.4201 - val_acc: 0.7692\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 12s 200ms/step - loss: 0.2223 - acc: 0.9147 - val_loss: 1.3669 - val_acc: 0.7511\n",
      "The highest validation acc is 0.7828054428100586\n",
      "CNN 5-Fold CV:\n",
      "Average Acc: 0.8002\n",
      "Average Precision: 0.8002\n",
      "Average recall: 0.8002\n",
      "Average F1 Score: 0.8002 \n",
      "\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 13s 203ms/step - loss: 2.4132 - acc: 0.4038 - val_loss: 6.3252 - val_acc: 0.1810\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.2210 - acc: 0.4917 - val_loss: 1.8035 - val_acc: 0.3167\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1420 - acc: 0.5209 - val_loss: 1.1483 - val_acc: 0.5023\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1247 - acc: 0.5103 - val_loss: 1.0437 - val_acc: 0.5566\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.1133 - acc: 0.5245 - val_loss: 1.2190 - val_acc: 0.4661\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0565 - acc: 0.5482 - val_loss: 1.1215 - val_acc: 0.5837\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0549 - acc: 0.5664 - val_loss: 0.9801 - val_acc: 0.5792\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 1.0110 - acc: 0.5972 - val_loss: 0.9619 - val_acc: 0.5792\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9950 - acc: 0.5946 - val_loss: 1.1255 - val_acc: 0.5837\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9490 - acc: 0.6219 - val_loss: 1.1605 - val_acc: 0.5068\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9397 - acc: 0.6159 - val_loss: 0.9560 - val_acc: 0.6335\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 12s 201ms/step - loss: 0.9118 - acc: 0.6345 - val_loss: 1.1485 - val_acc: 0.5520\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 13s 205ms/step - loss: 0.8987 - acc: 0.6401 - val_loss: 0.9428 - val_acc: 0.6244\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 14s 221ms/step - loss: 0.8642 - acc: 0.6754 - val_loss: 0.9053 - val_acc: 0.6561\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.8390 - acc: 0.6784 - val_loss: 0.9365 - val_acc: 0.6063\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.8314 - acc: 0.6916 - val_loss: 0.8681 - val_acc: 0.6742\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.7761 - acc: 0.7062 - val_loss: 0.8393 - val_acc: 0.7014\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.7393 - acc: 0.7375 - val_loss: 1.0387 - val_acc: 0.6606\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.6988 - acc: 0.7567 - val_loss: 0.9097 - val_acc: 0.6606\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.6976 - acc: 0.7516 - val_loss: 0.8232 - val_acc: 0.7059\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.6360 - acc: 0.7713 - val_loss: 0.7163 - val_acc: 0.7602\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.6340 - acc: 0.7708 - val_loss: 0.8557 - val_acc: 0.6968\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.5965 - acc: 0.7981 - val_loss: 0.8403 - val_acc: 0.6878\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.5536 - acc: 0.8107 - val_loss: 0.7342 - val_acc: 0.7557\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.5308 - acc: 0.8173 - val_loss: 0.8376 - val_acc: 0.7421\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.5216 - acc: 0.8198 - val_loss: 0.8768 - val_acc: 0.7692\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.4976 - acc: 0.8415 - val_loss: 0.7855 - val_acc: 0.7692\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.4901 - acc: 0.8375 - val_loss: 0.9371 - val_acc: 0.7014\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.4787 - acc: 0.8410 - val_loss: 0.8713 - val_acc: 0.7511\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.4498 - acc: 0.8516 - val_loss: 0.8014 - val_acc: 0.7873\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.3937 - acc: 0.8597 - val_loss: 0.8387 - val_acc: 0.7919\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.3981 - acc: 0.8688 - val_loss: 0.8517 - val_acc: 0.7873\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.3767 - acc: 0.8561 - val_loss: 0.9992 - val_acc: 0.7557\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.3607 - acc: 0.8753 - val_loss: 0.7879 - val_acc: 0.7964\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.3754 - acc: 0.8667 - val_loss: 0.8212 - val_acc: 0.8100\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.3237 - acc: 0.8894 - val_loss: 0.7950 - val_acc: 0.8009\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.3469 - acc: 0.8763 - val_loss: 0.9466 - val_acc: 0.7919\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.3080 - acc: 0.8930 - val_loss: 0.8732 - val_acc: 0.8235\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2998 - acc: 0.8955 - val_loss: 0.9240 - val_acc: 0.8100\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.3237 - acc: 0.8829 - val_loss: 0.8519 - val_acc: 0.7919\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2971 - acc: 0.8965 - val_loss: 0.8682 - val_acc: 0.8054\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2669 - acc: 0.9056 - val_loss: 0.9493 - val_acc: 0.8145\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2855 - acc: 0.8940 - val_loss: 0.9396 - val_acc: 0.8281\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2441 - acc: 0.9001 - val_loss: 1.0346 - val_acc: 0.7919\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2555 - acc: 0.9066 - val_loss: 0.9639 - val_acc: 0.8281\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2240 - acc: 0.9031 - val_loss: 1.0313 - val_acc: 0.7964\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2149 - acc: 0.9096 - val_loss: 1.1043 - val_acc: 0.7783\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2166 - acc: 0.9107 - val_loss: 1.1427 - val_acc: 0.7828\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2555 - acc: 0.9076 - val_loss: 1.2092 - val_acc: 0.7964\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 12s 198ms/step - loss: 0.2152 - acc: 0.9081 - val_loss: 1.0155 - val_acc: 0.8281\n",
      "The highest validation acc is 0.8280543088912964\n",
      "CNN 5-Fold CV:\n",
      "Average Acc: 0.7976\n",
      "Average Precision: 0.7976\n",
      "Average recall: 0.7976\n",
      "Average F1 Score: 0.7976 \n",
      "\n",
      "INFO:tensorflow:Assets written to: Task B Assets/Task_B_CNN_Model\\assets\n"
     ]
    }
   ],
   "source": [
    "#Uncomment and run to see full training process.\n",
    "#Model is saved as to ease model testing.\n",
    "\n",
    "model = train_CNN(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2724d-9d90-42b2-b35c-08771872b662",
   "metadata": {},
   "source": [
    "## Testing (using splitted sample data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afaf2c3a-1957-4685-9841-a139d0e3731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 4)\n",
      "Accuracy for test data: 0.8313953488372093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 504x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAFJCAYAAABeucAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABGuUlEQVR4nO3dd5gUVdbH8e9vhhwEJCgGBBVzQAEz5owr5rhmF3XNa8Kcd3Xd1+yuYgIUA4qrGBFREAMoYiCYMGBYJUgQJMN5/7h3oBmYoWe6e6q7OZ/n6Yfu6uqqUzNDnb5ZZoZzzjmXiZKkA3DOOVf4PJk455zLmCcT55xzGfNk4pxzLmOeTJxzzmXMk4lzzrmM1Uo6AJcM1W1satA86TCybqt2xXdNZUqkpENwVfDDhO+ZMmVKRr+00tXWM1s4J619bc7kgWa2fybny4Qnk1WUGjSn7h5XJx1G1r3R56SkQ8iZurWKsyKhVmlxJsmdd+ic8TFs0VzqbnpsWvvOHXV3i4xPmAFPJs45l89UGF8iPJk451w+K5DqTU8mzjmXt+QlE+ecc1ngJRPnnHMZEQVTMimMKJ1zbpUkKClN77GyI0mPSJokaUy57edK+kLSWEn/TNl+uaTxkr6UtN/Kju8lE+ecy2fZq+bqBdwL9Fl6aO0BdAO2NrN5klrF7ZsBxwCbA2sBb0jayMwWVXRwL5k451zeig3w6TxWwszeBqaW23wWcIuZzYv7TIrbuwFPmdk8M/sOGA9sV9nxPZk451y+EqFkks4DWkgamfLonsYZNgK6SBohaaikspGWawM/puz3U9xWIa/mcs65fJZ+A/wUM+tUxaPXAlYHdgA6A/0krV/FYyw5kHPOubyU83EmPwHPWVi//QNJi4EWwM/Auin7rRO3VciruZxzLl8JKC1N71E9zwN7AEjaCKgDTAEGAMdIqiupHdAe+KCyA3nJxDnn8lmWenNJehLYndC28hNwLfAI8EjsLjwfOCmWUsZK6geMAxYCZ1fWkws8mTjnXB7LXjWXmVU0/fCfK9j/ZuDmdI/vycQ55/KZT6finHMuYwUynYonE+ecy1dSWlOl5ANPJs45l8+8mss551xmfD0T55xz2eAlE7cquOesLuy37bpMmTGXnS5+bpn3zj5oC246cXs2OO1xps6cx7l/2pIju2wAQK2SEjZapwkbntaX6X/MTyL0KrnoH08w+L1xNG/WiMF9egBw20Ov8Pqw0ZSUiObNGnP7FcexZosmCUdafeMnTOQvV/da8nrCz1O47C8HcsYxeyQXVBYtWrSYPU+6jdYtm/DUHWcmHU56Cmg9E08mLiNPDvmaB18bx/1n77bM9rWbN2SPrdbmx8mzlmy758XR3PPiaAD277guZ3XdoiASCcCRB2zPyYd14YKb+y7Zduaxe3LJ6QcC8MizQ7mr10D+cfFRSYWYsQ3XW4O3+lwGhBvvVgdfzYG7bZ1wVNlz/1ND2KjtGsz8Y27SoVRB4VRzFUaUGZDUS9IR8flDcZ7+mjx/W0nH1eQ5a9J7n//KtFnzltt+80nbc13fDwmDaZd3+M4b0P/db3MdXtbs0GEDmq7WYJltjRvWW/J89pzCSIrpenvkl7RduwXrtl496VCy4ueJ0xj07lhO6LZj0qFUXZYWx8q1VapkYmanJ3DatsBxwBM1dUJJtcxsYU2dr7wDOrXhl6mzGTOh/NIJQf06pezVYR0uefi9Go4s+27t+TL9B35I44b16HfXOUmHkzXPDxrFYft0TDqMrLnijue47txuzJq9/BefvFcgbSZFVTKRdHVcYvIdSU9Kurjc+0MkdYrPj5U0WtIYSbem7DNL0m1xCcs3JG0XP/etpIPjPm0lDZM0Kj52qiSsWwjrBXwi6UJJJ0u6N+V8L0navQrnrifp0Rj7x3GlNOJxB0h6ExiclR9oNdSvU8rfDt2afzz9UYX77N+xDSO+nFgwVVyVuax7Vz7ofx2H7tORXs8NSzqcrJi/YCED3xnDn/bqkHQoWTFw2BhaNmtEh03bJB1K1Sl7i2PlWvIRZElc1OVwYGvgAKDCef0lrQXcCuwJdAA6Szokvt0QeNPMNgdmAjcB+wCHAjfEfSYB+5jZtsDRwN2VhNYDGGZmHczsjpVcRjrnPhswM9sSOBboLamsvmVb4AgzW7YBY+l1dy9bOMfmzVxJKNXTbo3VWK9VY4bddiif3nsUazVvyNBbD6FVk/pL9jls5/Xp/843OTl/Ug7dtxOvDP006TCyYvD749hy43VotfpqSYeSFSM++5ZXh41h627XcvqVjzJs5FeccU3vpMNKX/qLYyWqmKq5dgZeMLO5wFxJL1ayb2dgiJlNBpDUF9iVMB3zfOC1uN9oYJ6ZLZA0mlBlBVAbuFdSB2ARYbWybEjn3LsA9wCY2ReSJqScf5CZrbhuKezfE+gJUNKs7YobMzI07sdpbPSXpTV6n957FHtc/gJTZ4bqhdXq12bnzVpzxj1Dc3H6GvXdj5Npt25LAF4fNpoN26yRcETZ8d8iq+K65uyDuebsgwF456OvuffxwTxww0kJR5U+5UGiSEcxJZNsWWBLW40XA2VrIy+WVPbzuhCYSCgFlQBV6R6ykGVLhPVSnqdz7sr8UYU4suKh83dn581a07xxPcb85xhu6TeKx9/6qsL9u27Xlrc+/ZnZ8xJr0qmWs6/rzfCPv2HqjFl0PuxaLjr1AN4cPo5vfphEicQ6a67O3y8+MukwM/bHnHkM/eAL/nXZ0UmH4ihbtdeTSU17F3hA0j8I13UQ8Vv4CnwA3C2pBTCNUF10TxXO1QT4Kd7kTwIq60oxE2ic8vp74K+SSghrKm9XhfMCDAOOB96Mi9m0Ab4kVHHVuNPvGlLp+1uf02+Z108O/Zonh36dw4hy477rlv8me8xBOyQQSW41rF+XLwfeknQYObNLx/bs0rF90mGkT0IlnkxqlJl9KGkA8Bmh1DAamFHBvr9I6gG8RUj+L5vZC1U43b+B/pJOJFRLVVYi+AxYJOlToBdwJ/AdYdGZz4FRVThv2bn/E6u+FgInm9m8Qvn24pyrmkL5v62KxgEUIkmNzGyWpAbA20B3M6vqzXqVUNKsrdXd4+qkw8i6H/sUTl14VdWtVTT9ZZZRq7QwbpZVtfMOnRn10ciMLq509XbWcN/r09p35tMnfWRmFXY8yrVi++vsKekTwrf9/p5InHOFTlJajzSO84ikSXGJ3vLvXSTJYtU/Cu6WNF7SZ5JWWo1eNNVcAGaW2EhzSVsCj5XbPM/Mtk8iHudcEVB8ZEcv4F6gzzKnkNYF9gV+SNl8ANA+PrYH/hP/rVBRJZMkmdlowpgV55zLCiFKSrK2Bvzbktqu4K07gEuB1HbjbkCf2Lt0uKSmklqb2S8VHb/Yqrmcc66oZKuaq4JjdwN+NrPyI27XBn5Mef1T3FYhL5k451weq0KiaCFpZMrrnnGgckXHbQBcQajiypgnE+ecy1dVazOZUsXeXBsA7YBPY8JaBxglaTvgZ2DdlH3Xidsq5NVczjmXx3JVzWVmo82slZm1NbO2hKqsbc3sV2AAcGLs1bUDMKOy9hLwZOKcc3lLpJdI0uwa/CTwPrCxpJ8knVbJ7q8A3wLjgQeBv67s+F7N5ZxzeSxb06mY2bEreb9tynMjzFCeNk8mzjmXr1Q406l4MnHOuTzmycQ551zGPJk455zLSFkDfCHwZOKcc/msMHKJJxPnnMtbImtzc+WaJxPnnMtjXs3lnHMuc4WRSzyZOOdcPvOSiXPOuYxkMr18TfNk4pxzecwb4F1e23r9Fgx94pSkw8i61if0TjqEnJnw6AlJh5ATtUpLkw4hvxVGwcSTiXPO5TOv5nLOOZcZn+jROedcpgQUSC7xZOKcc/nLe3M555zLgpIsLY6Va4XR58w551ZFCtVc6TxWeijpEUmTJI1J2XabpC8kfSbpv5Kaprx3uaTxkr6UtN/Kju/JxDnn8pQIJZN0HmnoBexfbtsgYAsz2wr4CrgcQNJmwDHA5vEz/5ZUaR9uTybOOZfHslUyMbO3ganltr1uZgvjy+HAOvF5N+ApM5tnZt8B44HtKju+JxPnnMtjZVOqrOyRBacCr8bnawM/prz3U9xWIW+Ad865PCVVqQG+haSRKa97mlnP9M6jK4GFQN8qhriEJxPnnMtbVSp1TDGzTlU+g3QycBCwl5lZ3PwzsG7KbuvEbRXyai7nnMtj2WozWfGxtT9wKXCwmc1OeWsAcIykupLaAe2BDyo7lpdMnHMuj2Vr0KKkJ4HdCdVhPwHXEnpv1QUGxfMMN7MzzWyspH7AOEL119lmtqiy43sycc65fJVBqaM8Mzt2BZsfrmT/m4Gb0z2+JxPnnMtTYW6uwhgB78nEOefyWKFMp+LJxDnn8liBFEw8mTjnXN7y9Uycc85lytczcc45lwW+nolzzrksKJBc4snEOefyVtXm5kqUJxPnnMtTPs7EOeD+J9/iiRffB4lNN2jNXVceT726tZMOK213n7Ez+26zLlN+n8sulz6/zHt/7bo5N/55O9p3f4KpM+ex86Zr8vjFezFh0kwAXvpwAv967tMEoq6ai/7xBIPfG0fzZo0Y3KcHALc99AqvDxtNSYlo3qwxt19xHGu2aJJwpJlbtGgxe550G61bNuGpO85MOpy0FUoy8YkeXU78Mmk6Dz0zlIGPXMzbfS9n8aLFPP/GqKTDqpInh47nqFsGLbd9rdUbsseWa/Pj5FnLbH//i4nsfvkAdr98QEEkEoAjD9iex/51xjLbzjx2Twb1voyBj17K3jttxl29BiYUXXbd/9QQNmq7RtJhVFkuJ3rMpoJLJpJeSV2nuIqf7STp7iyHlM55T5a0Vk2fN2mLFi1m7rwFLFy4iNlzF7Bmi9WSDqlK3v9iItNmzVtu+80nbsd1T3yIYSv4VGHZocMGNF2twTLbGjest+T57DnzazqknPh54jQGvTuWE7rtmHQoVVaDi2NlpOCquczswAw+OxIYudIds+9kYAzwv5o6oaTSlc3ymUutWzXlrOP2ZNtDr6V+3drstt0m7L79pkmFkzUHdGzDL1NnM/aHacu917l9S4be0o1fp83mmr4f8uVP02s+wCy5tefL9B/4IY0b1qPfXeckHU7GrrjjOa47txuzZi//5SCfSWmv7564nJVMJLWV9IWkXpK+ktRX0t6S3pX0taTtJDWU9IikDyR9LKlb/OzJkp6T9Frc958px/1eUot4/M8lPShprKTXJdWP+3SW9JmkTyTdJmlM3L67pJfi89UlPR/3Gy5pq7j9Okm9JQ2TNEHSYZL+KWl0jKd23O8aSR9KGiOppyr4aiDpCKAT0DfGU7/sGuL7nSQNqeK594o/r9Hx51c35Wdzq6RRwJEriKW7pJGSRv42eXI2fs0Vmv77bF4bNpoP+1/Lpy/exOy583n2tQ9zes5cq1+nlAsP2Yp/PLN8dd1n3/9Gh3OfYbceL/DgwM957G97JRBh9lzWvSsf9L+OQ/fpSK/nhiUdTkYGDhtDy2aN6LBpm6RDqRav5go2BP4P2CQ+jgN2AS4GrgCuBN40s+2APYDbJDWMn+0AHA1sCRwtaV2W1x64z8w2B6YDh8ftjwJnmFkHoKJv59cDH5vZVjGWPinvbQDsCRwMPA68ZWZbAnOArnGfe82ss5ltAdQnrFS2HDN7llAaOt7MOpjZnAriSevckuoBvYCj4/ZawFkpn//NzLY1s6dWEEtPM+tkZp2at2y5kjAy8/aHX9KmdXNaNGtM7VqldN1taz4c/V1Oz5lrbddYjTYtG/H2rd34+O4jWGv1hrz194Np1aQ+M+cs4I95CwF445OfqF1LrN64bsIRZ+7QfTvxytDCaP+pyIjPvuXVYWPYutu1nH7lowwb+RVnXNM76bDSViKl9Uharqu5vjOz0QCSxgKDzcwkjQbaEpaCPFjSxXH/ekDZ14fBZjYjfnYcsB7LLnBfdvxP4vOPgLaxPaWxmb0ftz/Bim/0uxCTj5m9Kam5pLJK/VfNbEGMsxR4LW4vixtgD0mXAg2A1YGxwItp/VQqt7Jzb0y47q/i9t7A2cCd8fXTWYghY2uv2YxRY79n9tz51K9bm2Ejv2LrTVf0faBwfP7jNDY5c2mO/vjuI9jryheZOnMerZrUZ9KM8D1h2w1aUCIxdWZhVamU+e7HybRbN3zZeH3YaDZsU3iN1qmuOftgrjn7YADe+ehr7n18MA/ccFLCUaUvD/JEWnKdTFL/Ny1Oeb04nnsRcLiZfZn6IUnbl/vsIlYca/l96mcacOpxzWyxpAUp6yIvBmrF0sG/gU5m9qOk6wiJMF0LWVoqLP+5Ss+dxrH/qEIcOdNx87YctEcH9jnpn5TWKmXLjdbmhG47JR1WlfQ8dzd23nRNmjeux+h7j+KWZz+m75CvV7jvwdu35ZR9NmbhImPu/IWcfvfQGo62es6+rjfDP/6GqTNm0fmwa7no1AN4c/g4vvlhEiUS66y5On+/eLkaU1dD5BM9pm0gcK6kc2OJZRsz+ziTA5rZdEkzJW1vZiOAYyrYdRhwPHCjpN2BKWb2e5q/uLIEMEVSI+AI4NlK9p8JNE55/T3QEXiVpVVz6fqSUALb0MzGAycAeXnnuvQvB3LpX6rdXyJx3e+p/Me6zXlLf+UPvf45D73+ea5Dyrr7rlv+G/oxB+2QQCQ1Y5eO7dmlY/ukw6iSAml/rziZSLoHKu77aGbnZeH8NxKqZz6TVAJ8RwVtD1V0GvCgpMWEG+2MFexzHfCIpM+A2UDa5d6YsB4k9ND6FVhZy3Iv4H5Jc4AdCe01D0u6ERiS7nnjuedKOgV4RlKteO77q3IM51zhyFZvLkmPEO6vk2JbL5JWJ1SNtyV8yT3KzKbFDkV3AQcS7o8nm1mlA8W0tBZluRNXenM1s7xtwZLUyMxmxec9gNZmdn7CYeWVbTp2sqHvfpB0GFnX+oS8/bPM2IRHT0g6hJxoWLc06RByYucdOjPqo5EZZYKm621qu1zRZ+U7Ai+fud1HZtapovcl7QrMAvqkJJN/AlPN7JZ4r2xmZpdJOhA4l5BMtgfuMrPtKzt/hSWT8slCUgMzm53WVSWvq6TLCdc3gTDOwznnCk62qrnM7G1Jbctt7gbsHp/3JtSUXBa394lttsMlNZXU2sx+qTDOlQUgacfYm+qL+HprSf+u6oXUJDN7OnbD3cLMuppZbgdVRJLui2NJUh+n1MS5nXNFKM3R77Gtt0XZOLL46J7GGdZISRC/AmVd99Zm2d6zP8VtFUqnAf5OYD9gAICZfRqLS64cMzs76Ricc8WlCp25plRWzbUysRNUtecISqs3V+z+mropsWk6nHNuVSHI9YDEiWXVV5JaA5Pi9p+B1IFh68RtFUpnBPyPknYCTFLtOMCw8PpAOudcASopUVqPahrA0p6sJwEvpGw/UcEOwIzK2ksgvZLJmYQuYmsTJiocSBhx7ZxzLoeyOe+WpCcJje0tJP0EXAvcAvSTdBqhs9JRcfdXCD25xhO6Bq+07XelycTMphAG9znnnKth2armMrNjK3hruVlJYy+uKhUa0unNtb6kFyVNljRJ0guS1q/KSZxzzlWP0nwkLZ02kyeAfkBrYC3gGeDJXAblnHMuqELX4ESlk0wamNljZrYwPh6napMaOuecqwZJlJak90haZXNzrR6fvhqH2T9FmKvraELjjHPOuRzLg0JHWiprgP+IkDzKLuWMlPcMuDxXQTnnnAvyoQorHZXNzdWuJgNxzjm3rDBoMeko0pPWCHhJWwCbkdJWYmbpTWXpnHOu2gq+ZFJG0rWEgS6bEdpKDgDeYdk1051zzuVAYaSS9HpzHUEY1PKrmZ0CbA00yWlUzjnnkCj83lwp5sT1yBdKWo0wEdi6K/uQc865zBVNNRcwUlJT4EFCD69ZwPu5DMo551xQILkkrbm5/hqf3i/pNWA1M/sst2E555wTyvUU9FlT2aDFbSt7b2WLyzvnnMtQFmcNzrXKSib/V8l7BuyZ5VhcDTIzFixanHQYWffVg8U7wfWf7ns36RByou+p2yUdQk5k6/9XwbeZmNkeNRmIc865ZQkoLfRk4pxzLnl50Os3LZ5MnHMujxVKMkln0KJzzrkEhGV7s7OeiaQLJY2VNEbSk5LqSWonaYSk8ZKellSnurGms9KiJP1Z0jXxdRtJxdli5pxzeaZE6T0qI2lt4Dygk5ltAZQCxwC3AneY2YbANOC0aseZxj7/BnYEytYPngncV90TOuecS4/I6nQqtYD6kmoBDYBfCL1yn43v9wYOqW6s6SST7c3sbGAugJlNA6pdFHLOOZe+kjQflTGzn4F/AT8QksgMwowm081sYdztJ2DtTOJcmQWSSgljS5DUEii+AQrOOZeHpPQeQAtJI1Me3ZceQ82AbkA7YC2gIbB/NuNMpzfX3cB/gVaSbibMInxVNoNwzjm3PKlK06lMMbNOFby3N/CdmU2Ox30O2BloKqlWLJ2sA/xc3VjTmZurr6SPCNPQCzjEzD6v7gmdc86lL0tjFn8AdpDUAJhDuJ+PBN4iFBCeAk4CXqjuCdJZHKsNMBt4MXWbmf1Q3ZM655xLTzbGmZjZCEnPAqOAhcDHQE/gZeApSTfFbQ9X9xzpVHO9TGgvEWHZ3nbAl8Dm1T2pc865lSvrzZUNZnYtcG25zd8CWRnqkU4115apr+Nswn+tYHfnnHPZksYYknxR5elUzGyUpO1zEYxzzrllqUBWgU+nzeRvKS9LgG2B/+UsIuecc0Co5iqmkknjlOcLCW0o/XMTjnPOuVRFkUziYMXGZnZxDcXjnHMuRcEvjlU2kEXSzjUZkHPOuUCC0gKZ272ykskHhPaRTyQNAJ4B/ih708yey3Fszjm3yqvCCPhEpdNmUg/4jTC7ZNl4EwM8mTjnXA4VSwN8q9iTawxLk0gZy2lUzjnngKxNp5JzlSWTUqARrLCTsycT55zLOVFSBONMfjGzG2osElcULvr7E7zx3jhaNGvE4Md6AHDjfS/wxrtjqV27lPXWasHtVxxLk8YNEo60anrc+hRvDh9H86aNePXRSwEYN/5nrr79GebPX0hpaQnXX3A4W2+6XsKRrtyl+23EDus3Z/rsBZzaeyQAu23UgpN3bEub5g04q+8ovpo4C4CO6zWje5d21CoRCxcb9w/9lo9/nJ5g9On5ZdJ0Lr/tKX6bNhNJHHng9pxwaJcl7/d6dii39XyJd565jmZNGiYYaeUKqQG+sjALIx0WAEknS1or6ThqwpEHbs/j/3fGMtt27bwxg/tcxhu9L2P9dVty72NvJBRd9R22f2ceubX7MttufeBFzjtpP1586GIuOGV/bn3gpYSiq5rXxkzksv6jl9n23ZTZXDNgLJ/9NGOZ7TPmLOCK/47htD4f8Y9Xv+DyAzapyVCrrVZpCZd2P4gXH7qEJ+86hycHvMf4CROBkGje/egrWrdqmmyQaSqJ09Cv7JG0ypLJXjUWRfE7mbAgTY2JY4Rq3A4dNqDpasuWOnbbbhNq1QrhbLt5W36ZPGNFH81r2229/HUJMeuPuQDM/GMuazRfLYnQquyzn2fw+9wFy2z7Yepsfpw2Z7l9x0+axW9/zAfg+99mU7dWCbVLk79xrUzL5quxWft1AGjYoB7rt2nFpCnh7+7W+wdw0eldC2L8hqjS4liJqjCZmNnUmgwkX0lqK+lzSQ9KGivpdUn1JXWQNFzSZ5L+G1cyW9HnjwA6AX0lfRI/+72kFvH9TpKGxOfXSeotaZikCZIOk/RPSaMlvSapdtxvL0kfx+2PSKobt38v6VZJo4Aja+LnU1VPvzyCPXbYNOkwsuKqcw7hlgdeZJejbuCW+wdw8V+6Jh1STu3avgVfT5rFgkWF1WT6869T+Xz8/9hqkza8+d4Y1mjRhE02KJyKgmIombil2gP3mdnmwHTgcKAPcJmZbQWMZvmpnQEws2cJi9Acb2YdzGz5r3/L2oDQDftg4HHgrThz8xygq6R6QC/g6Li9FnBWyud/M7Ntzeypal1pDt3d+3VKS0s4bN+OSYeSFU+88C5X/rUb7/S7hiv+egiX3/Z00iHlTNvmDei+6/rcPuirpEOpkj/mzOOCG/rQ46yDKS0toeeTb3LOSfsmHVaVFHzJxC3jOzP7JD7/iHDDb2pmQ+O23sCuWTrXq2a2gJCgSoHX4vbRQFtg4xhP2f/q8ueu8I4mqXvZ+tC/TZmSpXDT0++VEbzx3ljuvfaEgqheSMdzr49kv123AuDA3bfm0y+Kc724Fo3qcMPBm3PLq1/wvxlzkw4nbQsWLuKCG/rQdc9t2GeXLfnxl9/4+depHHbmHexzwt+ZOHkGR/z1TiZP/T3pUCskwk06nUfSqjwF/SpqXsrzRUDTDI+3kKW//3orOpeZLZa0wMzK6hQWk97v64+K3jCznoTV1eiwbccaq6t4a/jn/OeJN3n2nnOpX69OTZ0259ZovhojPv2GHTpsyPujvqbt2i2TDinrGtYt5ZZDt+TBYd8x5n/5e9Mtz8y45vZ+rN+mFScfsRsAG7VrzbBnrluyzz4n/J1+956f1725wnomhfHly5NJ9cwApknqYmbDgBOAoZXsP5NlZ1/+HugIvEqoMquKL4G2kjY0s/FpnLtGnX1tb97/5BumTp9Fp0Ov5aLTDuDex95g/oKFHHvhv4HQCH/LJUclHGnVXHDjY4z4ZDzTZvzBzkdez/kn78fNFx/Fjfc8z6JFi6hbpzY3X5SXzVTLuarrpnRYpwlN6temX/cd6PXe9/w+dwHn7dmeJvVr849Dt+SbybO4tP9oDu2wNms1q8+JO67HiTuGbs+XPPsZ0+csWMlZkjVq7PcMeGMUG7Vbk8POvB2AC049gF23K6z2ujAC3pNJsTsJuF9SA8LSl6dUsm+vuO8cYEfgeuBhSTcCQ6pyUjObK+kU4BlJtYAPgfurHn5u3Hf9ScttO/agHRKIJLvuvPqEFW5/oeffVrg9n9308ucr3P7O+N+W2/b4iB94fEThVd913KIdY1+/rdJ9Bj12RQ1Fk5lspRJJTYGHgC0IA89PJXw5fZpQhf49cJSZTavO8T2ZrISZfU/44Ze9/lfK22ndJc2sP8uuATMM2GgF+11X7nWjFb1nZoOBbVbw+bbpxOOcKxxZLJjcBbxmZkdIqgM0AK4ABpvZLZJ6AD2Ay6pz8Hxot3HOObdCQkrvUelRpCaEjjoPA5jZfDObDnQjdOIh/ntIdSP1kkkWSboPKL/+y11m9mgS8TjnCpuA0vSLJi0kjUx53TN2ugFoB0wGHpW0NaFX6vnAGmb2S9znV2CN6sbqySSLzOzspGNwzhWXKtRyTTGzThW8V4uwPtW5ZjZC0l2EKq0lzMwkVbuXp1dzOedcvhJZqeYCfgJ+MrMR8fWzhOQyUVJrgPjvpOqG6snEOefyVLYGLZrZr8CPkjaOm/YCxgEDCD1Tif++UN1YvZrLOefyWBZnjDiXMEdgHZYOZygB+kk6DZgAVHsAmCcT55zLY9lKJXFKqBW1qWRlhnhPJs45l6eq2JsrUZ5MnHMujxVILvFk4pxz+UuoQBa99WTinHN5zEsmzjnnMhK6BhdGNvFk4pxz+SpPVlFMhycT55zLY76eiXPOuYyExbGSjiI9nkyccy6PeW8u55xzGSuQWi5PJs45l8+8ZOKccy4jQj6dinPOuQx512DnnHPZUCC5xJPJqqpUomHd4vv1N6ybdAS58/r5XZIOISda7Xhe0iHkxLyvfsr4GKFrcGGkk+K7mzjnXBEpjFTiycQ55/JbgWQTXwPeOefyWImU1iMdkkolfSzppfi6naQRksZLejou6Vu9OKv7Qeecc7mnNB9pOh/4POX1rcAdZrYhMA04rbpxejJxzrl8lqVsImkdoCvwUHwtYE/g2bhLb+CQ6obpbSbOOZenQp5Iu9zRQtLIlNc9zaxnyus7gUuBxvF1c2C6mS2Mr38C1q5urJ5MnHMuX1Vt0OIUM+u0wsNIBwGTzOwjSbtnJ7hleTJxzrk8lqXOXDsDB0s6EKgHrAbcBTSVVCuWTtYBfq7uCbzNxDnn8paQ0ntUxswuN7N1zKwtcAzwppkdD7wFHBF3Owl4obqRejJxzrk8JqX3qKbLgL9JGk9oQ3m4ugfyai7nnMtTVez2mxYzGwIMic+/BbbLxnE9mTjnXD4rkBHwnkyccy6P+eJYzjnnMlZSGLnEk4lzzuWtXDSa5IgnE+ecy2NezeWccy4jwpftdc45lwUFkks8mTjnXF4rkGziycQ55/KYrwHvnHMuY4WRSjyZOOdcfiuQbOLJxOXE3HkL6Nr9TuYtWMiihYs4eK9tuPyMrkmHlRXFem3jJ0zkL1f3WvJ6ws9TuOwvB3LGMXskF1QV3HP18ey3yxZMmTaTnY75+5LtfzlqN04/sguLFhuD3hnDtfe8QK3SEu6+6ni23mRdSktLePqVD7ij1+sJRr9iVVwcK1GeTFxO1K1Tixf+cx6NGtRlwcJFHHD67ey902Z03rJd0qFlrFivbcP11uCtPpcBsGjRYrY6+GoO3G3rhKNK35MvDefBfkO5//oTl2zbpWN7DtxtS7ocdwvzFyykRbNGAByy97bUrVOLnY/9O/Xr1mZ4v6t4duBIfvxlalLhr1hmMwLXqIKfgl7SQ5I2i8+vSPMzN0jaOz6/QFKDHMeY83PkG0k0alAXgAULF7Fg4aKVrrlQKIr52sq8PfJL2q7dgnVbr550KGl77+NvmPb77GW2nXp4F+7sPYj5C8LKtFOmzQLAzGhQvw6lpSXUq1eH+QsWMfOPuTUeczqytAR8zhV8MjGz081sXHyZVjIxs2vM7I348gKgSjd6SaVV2b8658hUNWLMukWLFtPluH+w0b492H37Tei0RdukQ8qaYr42gOcHjeKwfTomHUbGNlyvFTt22IBBj17MSw+czzabtQHghcEfM3vOfL549WZGv3gD9/YdzPRyiSg/ZGdxrJpQMMlEUltJX0jqK+lzSc9KaiBpiKROkm4B6kv6JO7TVtKYlM9fLOm6+LyXpCMknQesBbwl6a343n8kjZQ0VtL1KZ//XtKtkkYBPeK/Ze+1T31dLu4VnWNWyvtHSOqVEtd/JA2X9K2k3SU9Eq+3V8pnjpU0WtIYSbembJ8l6f8kfQrsWP2fdnaUlpYw7InLGfvyTYwaO4Fx4/+XdEhZU8zXNn/BQga+M4Y/7dUh6VAyVqu0hGarNWSfU/7FNXc9z6N/PxWAjpu3ZdHixWx6wJV06HYtZx+/J+ut3TzhaFcsx4tjZU3BJJNoY+DfZrYp8Dvw17I3zKwHMMfMOsTlKFfKzO4G/gfsYWZlrYxXmlknYCtgN0lbpXzkNzPb1sxuBmZI6hC3nwI8WoVzVKYZIRFcCAwA7gA2B7aU1EHSWsCtwJ5AB6CzpEPiZxsCI8xsazN7p/yBJXWPiXLk5CmT0wglO5o0bkCXjhsx+P1xK9+5wBTjtQ1+fxxbbrwOrVZfLelQMvbzpOm8+NYnAIwaN4HFZjRv2ogj9u/E4PfGsXDRYqZMm8WIT79lm03bJBvsCqRbxZUHuaTgksmPZvZufP44sEsOznFULGV8TLiJb5by3tMpzx8CTonVSUcDT2Tp/C+amQGjgYlmNtrMFgNjgbZAZ2CImU02s4VAX2DX+NlFQP+KDmxmPc2sk5l1atmiZZbCXbEp02YyY2aoNpgzdz5vffAF7duukdNz1pRivjaA/xZJFRfAK0M+o0unjQDYoE0r6tSuxW/TZ/HTr1Pp0nljABrUq0OnLdry9fcTkwy1YlnIJpLWlfSWpHGx1uX8uH11SYMkfR3/bVbdMAutN5et5HWqhSybLOut7OCS2gEXA53NbFqsWkr93B8pz/sD1wJvAh+Z2W8rO36K1LjLxzUv/rs45XnZ61rAgkqOO9fMFlUhjpz5dcrv/PW6x1i0eDGLFxuH7r0t+3fZMumwsqKYr+2POfMY+sEX/Ouyo5MOpcoeuulkdu7YnuZNGzHmpRu5pecrPD7gfe695njee+oK5i9YxFnXPRb2feZt7r3mz7z39JUIeOLF4YzN06rKLHUNXghcZGajJDUGPpI0CDgZGGxmt0jqAfQgrAtfZYWWTNpI2tHM3geOA94B/pTy/gJJtc1sATARaCWpOTALOAh4bQXHnAk0BqYAqxESxgxJawAHENdKLs/M5koaCPwHOG0lcaeeA2CipE2BL4FD4/vp+gC4W1ILYBpwLHBPFT5fI7ZovzZv9+2RdBg5UczX1rB+Xb4ceEvSYVTL6Vf1WuH2M67ps9y2P+bM55TLH8lxRNmRjcWxzOwX4Jf4fKakz4G1gW7A7nG33oT7XbWSSaFVc30JnB1/EM0IN/JUPYHPJPWNCeUGws13EPBFBcfsCbwm6S0z+5RQvfUFodrq3Qo+U6YvocSwstFOS84RX/cAXgLeI/6C0xX/KHoAbwGfEkpFL1TlGM65ApFm43tVGuAltQW2AUYAa8R7CsCvQLXraxWq5/Nf/AG8ZGZbJB1LGUkXA03M7OqkY6mqjh072bsjRiYdhquCBQsXJx1CTrTa8bykQ8iJeV/2Y/HsSRmVK7bapqO98ub7ae277up1J7C09gOgp5n1TN1HUiNgKHCzmT0nabqZNU15f5qZVavdpNCqufKGpP8CGxB6VTnnXNZVcXGsKbEn6oqPJdUmtPX2NbPn4uaJklqb2S+SWgOTqhtrwSQTM/seyJtSiZkdWn5bTDDl59S4zMwG1kxUzrlik43md4VRjQ8Dn5vZ7SlvDQBOAm6J/1a7yrxgkkkhWFGCcc65TGRpQOLOwAnAaEmfxG1XEJJIP0mnAROAo6p7Ak8mzjmXx7IxVUocxFzRgfbK+AR4MnHOubyWD6Pb0+HJxDnn8lS+zLuVDk8mzjmXx3xxLOecc5krjFziycQ55/JZNqZTqQmeTJxzLm/Jq7mcc85lpooj4BNVaBM9Ouecy0NeMnHOuTxWKCUTTybOOZfHvM3EOedcRiTvzeWccy4bPJk455zLlFdzOeecy5g3wDvnnMtYgeQSTybOOZfXCiSbeDJxzrk8JaCkQOq5ZGZJx+ASIGkyYZnOmtACmFJD56pJfl2FpyavbT0za5nJASS9Rog5HVPMbP9MzpcJTyYu5ySNNLNOSceRbX5dhaeYry1pPjeXc865jHkycc45lzFPJq4m9Ew6gBzx6yo8xXxtifI2E+eccxnzkolzzrmMeTJxzjmXMU8mzrlqk5aOqJPk95NVmP/yXUFLvZm5miVJFhtdJV0FdEw4JJcgTyauoJQlD0mbA9gq2oNE0raSNk0yhpREciSwiZl9mGQ8+UDSupLqJx1HEjyZuIJiZiZpN+BJSe2TjqcmlUukPYF5yUYEkk6PsbwXX9dONqKal/J72R54EmiUbETJ8GTiCkpMIFcCPczsa0mrzGSlMZHuCNwJ9DKzb2u6naJ8taKZPQQ8DhwvqZ6ZLVjVqh7j72Uv4ALgZjObvKr9DMCTiSsg8T/oJsDqwNGSSs1s4SrW8DsBWAPYA8DMFtfUicu1kVwg6UJJB5jZucBo4AVJDVfRqsfmwNHA+vF1aYKxJMIHLbq8VnYDk9QSWGBm0yXtCRwOfAvcYWaLJZXU5I21pqRc/6ZAHWA84Ub1BvCGmV2RQEyPAQ2BLwmJbaaZnS/pIWBroIuZza3puGpSyu+lFfC7mc2V1A3oB+xnZkOK9W+yIqvSNzpXgOJ/2D8BrxK++T4K/AwMBNoCPeJ/7KL8Txuv/2CgD3Aj8DDwJ2BvYA9J/1eT8cSk3tDMDjOzy4FrgXqSDjez04HXgPk1GVMSUv4uHwOej73Z3gMOA56VtHex/k1WxJOJy2uxjeQq4HRgH8CAvwKDCQmlDSGpFCVJTYFLCNd/LHA3cCiwTfx3f0kb5fD85ev+awGdJB0AYGY/At8BZb3rri4rKeYqpnwgaUPgn8BlwB3AbEKyfxX4GzBA0urF/nNItco0XrqCtQD4Ffifmc2PvYeGAd2Be4APzGxSkgHmWG1CtdZkM/tD0mfAB4SqpKGSOuaySimljeQk4BMz+1TShcDZkhaa2SBgTWJppKz6p1i/lae0G60OfGtmn8TtE4DtgD3NrI+kwWY2NcFQa9wqkzVdYUjpZllfUj1gIvAHsI2k1eJN6n5goZktLLZEknL96wKY2WRgCHCXpGZmNovwM9lAUh1gYQ3EdCdwLnCTpCuAX4BeQB9JTwLbAlfEeIuyETalhNYw/jsaqCPpLAAz+4JQ/bpZfP/Xcp8ret4A7/KOpEOBUwlVB1cD6wAXAh8Bk4HzgXPM7PXEgswhSfsTuv9+AgwAPiPUxR8F3AdcBJwVSwW5OH9qr631gLPN7FJJ2wHdCONbHgAWEcZU/BCrtkrNbFEuYsoH8fdyDqFk+Bihs8HehKXanyGMtznZzN5LLMgEeTJxeSGld0wjwn/Mu4ANgWuAnQk9mfYFNgD+a2aDEws2B1KuvxlwO+HGtBGwFaFN4nHgQKAe8L2ZvZnLOOLzx+LmnYGN4xiSnYGDCdVv95jZd3Hfou65JKkdIYHcDRwJjAPeBiYRxpf8AQwysxeTijFp3mbi8kK8kXYBNgY+NrPXACQtIrSR/MnM7ijWm1a8/r0J4xTqm9n7wPuSjgG2B04mDFScnus4AGK7SCmhFPQE8CahnebdWL22KfB9yueK7neSkuA7AM2Al82sn6S3gcuBvYCnzOy0sr/L1GS8qvE2E5eolDaCjsAjQFdCl9fjJNU2s/8Afwfeit/ai/JvVmFk+wOEwW8HSboVwMyeIlTvtQea5PD8qbP/Hg4cAgwxs1/MbA9ghqRhMaa3zOzf8UZbtG0C8fr2AAYBZwDXSupiZr8CNwEtgFNS2vKKts0oHV7N5RKnMBXFOYSpKEZKOhdoR6ibfjaOcl83dkMtOpI2ITRgDzWzhyWtDQwH+ppZj7hPazP7pQZiaQfMIExZ0wx4qKwNQNIIQqnxzFzHkQ8kbQGcBzwRByGeQeiWfo6ZDYsDFluY2bhEA80TRfktzxWkbsCu8Xlvwuj23QlTVAD8D4qvd4ykUqAx0BTYS9L6ZvYzoWrrzLJBiTWUSLoALxLaqm4i9Bo7KJaaMLPtCTfTVcUhQCdgo1hKfoDQHb23pN3MbJInkqU8mbjEKEyjfmRsTD+Q0PW0m5n9Thjx/QXwMUBZL6FiqkaIgw37EaYluQr4DThEUlsz+x+hm+nLOTx/+cT8MaHx/xZCQvk/wiDR4+O3dFaRAYlbSDrNzG4izALcAdgu9lZ7iPDzKbo2okwV9R+Fy3tbAn9VmIrjNeAI4BFJR8WEck+Rf/P7nTBmoyehx9ZjhBH9x0lqZ2b/M7M3c1UaS2lsPzO+ngU8TRhDchvQmvBNfAwwNuVzRXkjlVQSS4rrA/vFgZr/IowZOQboEhNKz1jNVVSl5Ex5MnE1TlIbADPrTZhr6pRYQnmFML6kt6Q1kowxl+LYDWJD7o3AD4TOB2MI3aLXSd0/26UxSXVTntcGTpT0eDzXH8BLhC6v/YDGZnZ/sTe2R/VjCfhtQsl4D0IvuhuBmYTpbFYr27mYSsnZ4F2DXc6ldueV1Bq4WNK3ZnanmT0evw3eLKl+nIqiTRz5XRRitZDFG3ID4A1JD5rZP81soqQ7gH8TbmAnAWPMbEaOYqlPaIsZThgfMYQwjmRgHFdyoplNlfQNoQPEt2WfLcabZ0r339bA57G31mhJ7xC+bJ9BmNLnSmADM5uWZLz5zEsmLqfit+D9YxXCpsD+wIeERs2yqSh6E0Z5HyepVVkiKYZvwgpTwuwD1FeYonwfwqC3kyWdB0sa1z8iDEhsn6tEEs81h9A28zLQitBby8xsX0JX1yclDSA0PN9uZotisi8qkhpK6pzS/XceoQv6QEmbxfE8rxOmqzkcWMvMxicXcf7zkonLtTpAS+B9wuR4uwJTCBMD7irpcsKAuDrAdZYy11YxfBO2sM5FB+A6Qlfb88zsE0nHEW7c9QkLXh0I/MXMxlZ4sAylDKgbAvQH1iM0LI81s+lmdoCkroSJG3vb0nViinGKlNrANZJmEmYZON3M/inJgCEKa+bUIdwjL4o97FwlfJyJyzmFkd1PEUofXc1sThyA2IlQJ70JIZEU5VQUktYktENMI8yvNScmma0Jsx+vQRjL8FwNx3Uy8Gfg77Gh/3ign5ktiO8X+1xbBxB6a/Uzs+5aOor9MuAgwrxjN9b076VQeTJxNSKOYegM7AhcbmbjJa1pZr9Kamlx3exiKI2UF0sfDQlTcKxJGJw5TmHN9LmSasWBmTVy/annkfQXwroocwjznnUs5gSSStIGhNLZncDTZnZzynslhAGJk4r17zLbPJm4nEr9dht7cZ0KbEGoZjkUuCCOqVglSLqXMEhxOGFG5N3M7OsaOO8yN8RyCWUHwjQur8U2kqKc/6wicbzP88CDhB51PYDDctl2VYw8mbgaJWkt4DhCv/3rzOylhEOqEeV6tF1MKKEMzVXVXkXJo1wcy33jLuZEUsHPoDQm0PaEAZu1CJ0S+icabAHyZOJyprIbk6SmZja92KsQypUAUktpdSysHCnIXWcDSTcTGpLnAQ+b2Xflbqapz5sVc9dXSRuY2Tcr2F6WUBoA9WLX6KL+u8wF7xrssqLspiipTSx9lE29sVyPwfifd3rZy5qLMndSrn9rSZ1jo3vZzLNlCaMskZSUJZLYLTdXieQBwpoowwk/52cV5v5KLZmUPT+WsBRv3QoPWMBiongs9lZbRkwkMrPZFpfa9URSdZ5MXFbEm2ZXQq+lqyW9ELcvTE0oKd8Cm0jatliqVOL1Hww8ShjodrOkg1LeK0s2pTHJrkYYV1MnWzGUnSNFHeAMM+tvZlcCLwAXSKoV4ygrMR1LaCfob2bzshVPPjGz2cCzxBHsWn5+sZK4vaGkLWs4vKLgycRlhaSdCBPgdQNGEGabfR+WJpTURAK8BjRILuLsijegCwmrQX5AmPG4q8JAxbKEUqvc9Y83s/lZOn9qddpOCtOjbwWckLLbMELV9sKUUtKfgb8Bx5rZ59mIJZ8oTNpYdp/7FLg+tXQW9yn7u2wKFNUKnjXJk4nLlunA8YRFnM4gTKleKzWhpNxI+wOXmNk7CcWaC/MJyWQr4EzCpJUCLpR0bLzZL4w3rP7AZWY2IhsnLpdIHiSUji4h3BgvUViHA8LAyCYpn+sMnEJYt7zoJtSUtBVhosZBko4mzEJ9K2EWgiUTO6YkkmeAS81sdFIxFzIfAe+qJaVnzGqEL97jFKbd6A48YmYzFeZ6ulRh2ooPJTUm3OAuLPREknL97QlrrXwVX18I9DSzjxUWk2pEWFDKJDUkDN68wcyGZSuWlERyMWENkk2A0wgzD7xO+B1sSWg/OTDlo2OA4y1MOFkUUn4vHQnrtR9KmLBxLcKKiRMJc209kNJe1AR4Drg2m7+XVY335nLVJukwwrfx6cBjFtbHvoJQKvmKMNr7/LLqE4U1Meqa2UfJRJxdkvYlTND4OfAqocTRmVA3fylhIakzzGxo3H99oGEuvvkqzHs2nJDIL4wN6YcD6xLaCW4HpsYbbSkhBxVFe1V5knYFTgfeMLM+Kdu3IKzbfhLQy8zujtu7AZMtrijpqsdLJq5KUr75ld2sriFMUPiEpHmEKpYz43v3p9bDm9mYJGLOppTrb0xoHzqaMOBvP0KV0T2EMTR7EAZkDk353LcVHDZjZva5wvobd0oabmZPS3oqxjSXpYlEVvwj3BsTpkOZAEsb2+Pf3xhJXwNbl+1sZi8kEWSx8ZKJqzKFWVY7E771XhDrnPcmLKzU3cz6K2UcRbF1s1SY02lLQiN7NzNbIGl/Ql3874TSypSUm3eNXb+kAwmz395qZk+Wa08put8FLJPgWwG/W5iiZm/CiPYLzOyFcj+HCwhrk+wOzCvWElpN8wZ4l5aUrq1bE759bwZsA5ymMADxDcKkgY9JWhsoumV2ASRtR2jUnUVog3gKwMJKkW8RSimrlV13TV+/hQXGLgfukLRP6vmL7XdRJiaSbkBv4FFJR8S/xzOA2xQWXku99snAKWY2xxNJ9njJxKVN0m7ADcBVFpYtPZlQXTCWMEZhmqTmZvZbknHmisIcTlcCY83sn3HbR8A3ZnZUfN3CzKYkGCYxjh2BD1aBKq2ykvJtQFfCuvVbETpB3Ksw9ul+wgzVk4o1oeYDL5m4qpgE7ERoE4DwTfBjQpXX0QqDE6dDcSxstQJNCN19d5K0DYCZdQS2kvR83CcvEqmZvW/Fu7BV+WtqQejs0JnQNf0+4CRJl5rZy8A2ZjbRE0lueQO8q1BKXXQbYHFs5N0Y+EDShPjtvE9MIsPNbGHZZ4vhP27K9W9OqLb7gdBL6xLCoMxFZvaZmW0iaRfIv+sutpJJ/Fs7XtJAQhformZ2SewQcR5wgpl9FdtMOiosAf1DkjGvKjyZuBVKuZF2Ay4GFkn6BHgc2BZ4T1JdM7vRzB5JMtZc0NKFkvYjjFcYChxAKJX1JHQ9PSbu94kV+LiZQiCpkZnNkvQ7MI7Q9vFnAAvjmuoSpovpRViS+FJPJDXHq7ncMiTVhiWNmmsRuv52j4+PCd1+5xO6wl4saf1iqkqR1ByWTFLZCrgKONPMuhPG1DxGWOjqYUKX6FlJxboqURihfomkFsArhM4ODQhLQJe5mlAV+R/C+vVZmWHApceTiVtC0hqEdbHrx011gflm9rmZfQW8QfibOcDCWuXrmNm3xVKVEq/7YknrAVhYj3488JvCtBvPElblu9DMviAs6To+sYBXIRZmme4J1CeMITmN0GttoMLiXhCqIv8M7FvWHTiJWFdVnkwcEEokZjaRMOiwpcLaD98BX0i6XmGJ2Z8JU3BsEP+jzomfLZb/tAsIk1WapKvitvmEyRLL2kK+IawNArGzgcutlEGHPwN7EuaA283MHgfK2u3OA94GOpT1Jsy39qti520mjlidc46kF8zsI0n3AWtLugjoBfwJ+K+kR4BzgFPjf9SFUBz/aWMb0UJghsI8Vl0kdSc06j4PPCLpV2B/QnVKUVx3IYhVjpsAu5pZT0nzgaNie9VDkiYRuv6eaGYfJxvtqsvHmThiPfStwC/A42b2haQ7CdNS/AuYQahWMOBDMxuYVKy5JGlnoJ2ZPa4wpf6lwMuE0lpXwlK7n5vZ28U6mjzfpHSEOJ4wRc17ZvZIfL0fYf2c5wi9DX0AYoI8mazitHQK7m0IpY7UhHIvoZHzZlvBcqfFIKXXWifgVMLU8ReY2RMxuVwIjDSzWxINdBUlaU0z+zV2DDkC2BEYbWYPSjqVMIXNBbGK1iXI20xWYfFGWpZIjgG+BNYg9OPfxMzOITRq3qyw7GnRiYlkH8JaFkMI3YAvkXSamb1LmDpme0kbJhjmKin24Bok6TgzW0CYjXkEcEj8/TwCXOSJJD94yWQVJ+lPhHEkdQmD8j4izDn1E/C0hXVKNrMiXDypjKSzgBIzuy92c96N0L30Gguz7za1pWvWuxyJX1j2MrMXJW1PWAumEXAt8PfYmw5JLxPGmFxtZj8mFrBbhpdMVmGxK/BlhDU3diAMzGtM6LG1PnCipIbFlkhW0PusLnAcLBkx/gEwitBN+ABPJLkXS8mzgWMljSKUCCdbmB7+euBqSccorNtSjzCOxBNJHvFksmqbT/gbaB5f9yRUc+0GvAf0NbM/EootZ2LVVhdJp0ja0MzuBL6RNCBO17EVYQ6ugYS5nlwOSVqXMLcWhOnz1ySMb/oMlqw30oMwruRh4L6y91z+8GquVVzs/tsA+K+ZjYnTh1xAqOY628zmJxlfNqU0tu9A6PL8BWGsyJuEVRL/Q5g0cD3Cole7AZsTfg7+HyVHJLUkLKv7KyGRzAYeISzo1S1lPwHNzGyq96bLP14ycU8DdQjrPtxMaIC+llBC2TTJwLJFUj1YUiLZgTBFyuFmdghhTfpOwCFmdiJhhchdCOunnw3c6zet3JFUy8wmm9mnhARytJl9bWZdCINn+0nqLGkQ0MrMpoKP8clHnkxWcWb2E2EtiLsIqwQeQajiaUv4pljQJDUjLCncOG5qDRxISBgQenGNAPaIDfFzCVN27AYcVWztRfnGzBZK2kVh3ZF+wI6SLlVYqXMnwsDqmwlJ3Xtt5TGv5nLLUFho6B+ERvlPk44nGyStQ6jKa2pmH0j6M3AFcFnsOVSXUK01six5KMyIPK/io7pMpFQ57kRoBxlFGOPUidDA/l/gTjObJ6mZhYXXvGorj3kyccuQ1BqoY2YTko4lU2Wjp+PzvxFGtP/JzD6UdBShUfdmC2vWl93clnzG5ZbCEsi3Apeb2fA4lqcr0AHYEHg/vlcUE4kWO5+byy3DzH5JOoZsiMlhsaTVzWyqmd0uaS7QV9KfzaxfHFV9k6RhhKnMzRNJjWoC7EqYvHE4MAH4jtAIfxWwhieSwuFtJq4oxVLGQcDTkl6KAy//DdwOPCppJzPrC+xhZpM8idQ8MxsEHAacKunYOMp9BmHOrT/MbFSiAboq8WouV5QkdSRMT34loVPBesDdZjZMYbryC4BtgN+9Hj5ZcRaGvsDrwGLC3HADko3KVZUnE1d0JK1NSCRmZn+O2y4n1MX/x8yGyNcGzyuSDgZuIAyUva1slgJP9IXDq7lcMZpDmBJlg9jQjpn9gzBI8YLYXdin4sgjsSRyKXC+pMMsSjoulz4vmbiCl9ITqwuwDmF8zAjCinydgNfNrH/cdwMr0un0i0GcwfkbM/s26Vhc1XjJxBW8mEj2BR4gLOA1mNDF9GVCCeXQlBKKJ5I8ZmaDPJEUJu8a7AqawvrgjQgLWx1NmPX4M+AdM/tF0rNAbeDz5KJ0rvh5NZcrSClVW3XjKOlTgS2AnYHjzOwbSacRRrUXxUh+5/KZl0xcQYqJ5BCgu6TxwJaEaThOi4lkK+Ai4KwEw3RuleHJxBWUlBJJU+Bk4PH41i7AusApktYCNgF6mNnQRAJ1bhXjycQVlJhItge2BT4ysycAJM0iTJ0/D7gaaGxmH/vkgM7VDG8zcQWh3CyzjwLjgVaEsQnvmNkCSScQZgPuZEW4QqRz+cyTiSsYsURyE/A3Mxst6UagKfAs8F5MKGub2c9JxuncqsjHmbhC0gTYA9gnvr4BmAqcRFzsyhOJc8nwZOIKhpm9TlhW9zRJx8VZZm8kjHiflGhwzq3ivJrLFRxJBxKSyD1m1ivhcJxzeDJxBSrOMnsLsDcw0RdRci5ZnkxcwZLU0swmJx2Hc86TiXPOuSzwBnjnnHMZ82TinHMuY55MnHPOZcyTiXPOuYx5MnFuBSQtkvSJpDGSnpHUIINj9ZJ0RHz+kKTNKtl39zj/WFXP8b2kFuluL7fPrCqe6zpJF1c1RlfcPJk4t2JzzKyDmW0BzAfOTH1TUrVm3Daz081sXCW77A5UOZk4lzRPJs6t3DBgw1hqGCZpADBOUqmk2yR9KOkzSWdAmOFY0r2SvpT0BmF2Y+J7QyR1is/3lzRK0qeSBktqS0haF8ZSURdJLSX1j+f4UNLO8bPNJb0uaaykhwCt7CIkPS/po/iZ7uXeuyNuHyypZdy2gaTX4meGSdokKz9NV5R8PRPnKhFLIAcAr8VN2wJbmNl38YY8w8w6S6oLvCvpdWAbYGNgM2ANYBzwSLnjtgQeBHaNx1rdzKZKuh+YZWb/ivs9AdxhZu9IagMMBDYlrN3yjpndIKkrcFoal3NqPEd94ENJ/c3sN6AhYXnjCyVdE499DtATONPMvo4zNv8b2LMaP0a3CvBk4tyK1Zf0SXw+DHiYUP30gZl9F7fvC2xV1h5CmNW4PbAr8GSc4uV/kt5cwfF3AN4uO5aZTa0gjr2BzaQlBY/VJDWK5zgsfvZlSdPSuKbzJB0an68bY/0NWAw8Hbc/DjwXz7ET8EzKueumcQ63ivJk4tyKzTGzDqkb4k01ddEtAeea2cBy+x2YxThKgB3MbO4KYkmbpN0JiWlHM5staQhQr4LdLZ53evmfgXMV8TYT56pvIHCWpNoAkjaS1BB4Gzg6tqm0JqzBUt5wYFdJ7eJnV4/bZwKNU/Z7HTi37IWkDvHp28BxcdsBQLOVxNoEmBYTySaEklGZEqCsdHUcofrsd+A7SUfGc0jS1is5h1uFeTJxrvoeIrSHjJI0BniAUNr/L/B1fK8P8H75D8YJKrsTqpQ+ZWk104vAoWUN8MB5QKfYwD+Opb3Kricko7GE6q4fVhLra0AtSZ8TZlsenvLeH8B28Rr2JCw6BnA8Ye2YT4GxQLc0fiZuFeUTPTrnnMuYl0ycc85lzJOJc865jHkycc45lzFPJs455zLmycQ551zGPJk455zLmCcT55xzGfNk4pxzLmP/DxA/s9xCdOyrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_57 (Conv2D)           (None, 128, 128, 32)      92288     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 64, 64, 64)        1493056   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 16, 128)       3612800   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 13,852,100\n",
      "Trainable params: 13,851,652\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.86      0.86      0.86       171\n",
      "meningioma_tumor       0.81      0.85      0.83       170\n",
      "        no_tumor       0.81      0.67      0.73       166\n",
      " pituitary_tumor       0.84      0.93      0.88       181\n",
      "\n",
      "        accuracy                           0.83       688\n",
      "       macro avg       0.83      0.83      0.83       688\n",
      "    weighted avg       0.83      0.83      0.83       688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266721f6-43fb-40b4-bb98-a9b4105f125c",
   "metadata": {},
   "source": [
    "# TO DO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8c460a-16eb-4c70-bdc2-49d2a1a37405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE sampling\n",
    "#add cross validation\n",
    "#tidy up functions\n",
    "#make version without SMOTE(task A makes sense, task B not so much)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
